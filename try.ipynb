{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EAST:\n",
    "    def __init__(self) -> None:\n",
    "        self.tfsession = None\n",
    "        self.graph = None\n",
    "        self.graph_def = None\n",
    "        self.image = None\n",
    "\n",
    "    def load_east_model(self, model_path) -> None:\n",
    "        # Load the EAST model from the provided path\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            # Load the model\n",
    "            saver = tf.compat.v1.train.import_meta_graph(model_path + '.meta')\n",
    "            saver.restore(sess, model_path)\n",
    "\n",
    "            # Get the graph definition\n",
    "            \n",
    "\n",
    "            self.tfsession = sess\n",
    "            self.graph = tf.compat.v1.get_default_graph()\n",
    "            self.graph_def = self.graph.as_graph_def()\n",
    "    \n",
    "    def load_image(self, imagepath) -> None:\n",
    "        self.image = cv2.imread(imagepath)\n",
    "    \n",
    "    def print_model_nodes(self) -> None:\n",
    "        # Print the names of input and output nodes\n",
    "        input_nodes = [node.name for node in self.graph_def.node if node.op == 'Placeholder']\n",
    "        output_nodes = [node.name for node in self.graph_def.node if 'Sigmoid' in node.name]  # Adjust this condition based on your actual output node names\n",
    "\n",
    "        print(\"Input Nodes:\")\n",
    "        for input_node in input_nodes:\n",
    "            print(input_node)\n",
    "\n",
    "        print(\"\\nOutput Nodes:\")\n",
    "        for output_node in output_nodes:\n",
    "            print(output_node)\n",
    "\n",
    "    def east_text_detection(self, conf_threshold=0.5):\n",
    "        # Get the input and output tensors from the graph\n",
    "        input_tensor = self.graph.get_tensor_by_name('input_images:0')\n",
    "        output_tensors = {\n",
    "            'scores': self.graph.get_tensor_by_name('model_0/feature_fusion/Conv_7/Sigmoid:0'),\n",
    "            'geometry': self.graph.get_tensor_by_name('model_1/feature_fusion/Conv_7/Sigmoid:0')\n",
    "        }\n",
    "\n",
    "        # Get image dimensions\n",
    "        height, width = self.image.shape[:2]\n",
    "\n",
    "        # Preprocess the image for EAST\n",
    "        image = cv2.resize(self.image, (320, 320))  # Resize the image to match the model's input size\n",
    "        image = image / 255.0  # Normalize pixel values to the range [0, 1]\n",
    "        image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Perform inference\n",
    "        scores, geometry = self.tfsession.run([output_tensors['scores'], output_tensors['geometry']], feed_dict={input_tensor: image})\n",
    "\n",
    "        # Decode the predictions\n",
    "        rectangles, confidences = self.decode_predictions(scores, geometry, conf_threshold)\n",
    "\n",
    "        # Return the detected rectangles and their confidences\n",
    "        return rectangles, confidences\n",
    "\n",
    "    def decode_predictions(self, scores, geometry, conf_threshold):\n",
    "        # Get the height, width, and number of scores\n",
    "        height, width = scores.shape[2:4]\n",
    "        num_scores = 5\n",
    "\n",
    "        rectangles = []\n",
    "        confidences = []\n",
    "\n",
    "        for y in range(0, height):\n",
    "            scores_data = scores[0, 0, y]\n",
    "            x_data0 = geometry[0, 0, y]\n",
    "            x_data1 = geometry[0, 1, y]\n",
    "            x_data2 = geometry[0, 2, y]\n",
    "            x_data3 = geometry[0, 3, y]\n",
    "            angles_data = geometry[0, 4, y]\n",
    "\n",
    "            for x in range(0, width):\n",
    "                score = scores_data[x]\n",
    "\n",
    "                if score < conf_threshold:\n",
    "                    continue\n",
    "\n",
    "                offset_x = x * 4.0\n",
    "                offset_y = y * 4.0\n",
    "\n",
    "                angle = angles_data[x]\n",
    "                cos_a = np.cos(angle)\n",
    "                sin_a = np.sin(angle)\n",
    "\n",
    "                h = x_data0[x] + x_data2[x]\n",
    "                w = x_data1[x] + x_data3[x]\n",
    "\n",
    "                offset = np.array([offset_x + cos_a * x_data1[x] + sin_a * x_data3[x],\n",
    "                                offset_y - sin_a * x_data1[x] + cos_a * x_data3[x]])\n",
    "\n",
    "                p1 = (-sin_a * offset_y + cos_a * offset_x - w // 2, cos_a * offset_y + sin_a * offset_x - h // 2)\n",
    "                p2 = (-sin_a * offset_y + cos_a * offset_x + w // 2, cos_a * offset_y + sin_a * offset_x - h // 2)\n",
    "                p3 = (-sin_a * offset_y + cos_a * offset_x + w // 2, cos_a * offset_y + sin_a * offset_x + h // 2)\n",
    "                p4 = (-sin_a * offset_y + cos_a * offset_x - w // 2, cos_a * offset_y + sin_a * offset_x + h // 2)\n",
    "\n",
    "                rectangles.append((p1, p2, p3, p4))\n",
    "                confidences.append(float(score))\n",
    "\n",
    "        return rectangles, confidences\n",
    "\n",
    "    def draw_predictions(self, image, rectangles, confidences):\n",
    "        # Draw the rectangles and confidences on the image\n",
    "        for i in range(len(rectangles)):\n",
    "            box = np.int0(rectangles[i])\n",
    "            confidence = confidences[i]\n",
    "            cv2.drawContours(image, [box], 0, (0, 255, 0), 2)\n",
    "            cv2.putText(image, f'{confidence:.2f}', (box[0][0], box[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        # Display the image\n",
    "        cv2.imshow('Text Detection', image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "east_model_path = 'eastmodel/model.ckpt-49491'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from eastmodel/model.ckpt-49491\n"
     ]
    }
   ],
   "source": [
    "east = EAST()\n",
    "east.load_east_model(east_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Nodes:\n",
      "input_images\n",
      "input_score_maps\n",
      "input_geo_maps\n",
      "input_training_masks\n",
      "\n",
      "Output Nodes:\n",
      "model_0/feature_fusion/Conv_7/Sigmoid\n",
      "model_0/feature_fusion/Conv_8/Sigmoid\n",
      "model_0/feature_fusion/Conv_9/Sigmoid\n",
      "model_0/gradients/model_0/feature_fusion/Conv_7/Sigmoid_grad/SigmoidGrad\n",
      "model_0/gradients/model_0/feature_fusion/Conv_8/Sigmoid_grad/SigmoidGrad\n",
      "model_0/gradients/model_0/feature_fusion/Conv_9/Sigmoid_grad/SigmoidGrad\n",
      "model_1/feature_fusion/Conv_7/Sigmoid\n",
      "model_1/feature_fusion/Conv_8/Sigmoid\n",
      "model_1/feature_fusion/Conv_9/Sigmoid\n",
      "model_1/gradients/model_1/feature_fusion/Conv_7/Sigmoid_grad/SigmoidGrad\n",
      "model_1/gradients/model_1/feature_fusion/Conv_8/Sigmoid_grad/SigmoidGrad\n",
      "model_1/gradients/model_1/feature_fusion/Conv_9/Sigmoid_grad/SigmoidGrad\n"
     ]
    }
   ],
   "source": [
    "east.print_model_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "east.load_image('azure2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\storagedrive\\ITB\\Learn\\Sem7\\TA1\\Code\\try.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/storagedrive/ITB/Learn/Sem7/TA1/Code/try.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m east\u001b[39m.\u001b[39;49meast_text_detection()\n",
      "\u001b[1;32mc:\\storagedrive\\ITB\\Learn\\Sem7\\TA1\\Code\\try.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/storagedrive/ITB/Learn/Sem7/TA1/Code/try.ipynb#W5sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(image, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# Add batch dimension\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/storagedrive/ITB/Learn/Sem7/TA1/Code/try.ipynb#W5sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Perform inference\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/storagedrive/ITB/Learn/Sem7/TA1/Code/try.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m scores, geometry \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtfsession\u001b[39m.\u001b[39;49mrun([output_tensors[\u001b[39m'\u001b[39;49m\u001b[39mscores\u001b[39;49m\u001b[39m'\u001b[39;49m], output_tensors[\u001b[39m'\u001b[39;49m\u001b[39mgeometry\u001b[39;49m\u001b[39m'\u001b[39;49m]], feed_dict\u001b[39m=\u001b[39;49m{input_tensor: image})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/storagedrive/ITB/Learn/Sem7/TA1/Code/try.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Decode the predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/storagedrive/ITB/Learn/Sem7/TA1/Code/try.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m rectangles, confidences \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_predictions(scores, geometry, conf_threshold)\n",
      "File \u001b[1;32mc:\\Users\\rkvilena\\miniconda3\\envs\\cnn-recognition\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\rkvilena\\miniconda3\\envs\\cnn-recognition\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1116\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[39m# Check session.\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closed:\n\u001b[1;32m-> 1116\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempted to use a closed Session.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mversion \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1118\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe Session graph is empty. Add operations to the \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1119\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mgraph before calling run().\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "east.east_text_detection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
